import { NextRequest, NextResponse } from 'next/server';
import Anthropic from '@anthropic-ai/sdk';
import { db } from '@/lib/db';
import { aiDoubles, users } from '@/lib/schema';
import { eq } from 'drizzle-orm';
import { uploadToBlob } from '@/lib/blob';
import { generateWav2LipVideo } from '@/lib/providers/wav2lip';
import { consumeQuota, getOrCreateUsage, updateSession } from '@/lib/visio/usage';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData();
    const audio = formData.get('audio') as File;
    const userId = formData.get('userId') as string;
    const sessionId = formData.get('sessionId') as string;

    if (!audio) {
      return NextResponse.json(
        { error: 'Audio requis' },
        { status: 400 }
      );
    }

    if (!userId) {
      return NextResponse.json(
        { error: 'UserId requis' },
        { status: 400 }
      );
    }

    const userIdNum = parseInt(userId);

    // 1. V√©rifier le quota
    const usage = await getOrCreateUsage(userIdNum);
    if (usage.remainingSeconds <= 0) {
      return NextResponse.json(
        { error: 'Quota de minutes √©puis√© pour ce mois', code: 'QUOTA_EXCEEDED' },
        { status: 402 }
      );
    }

    // 2. Speech-to-Text avec ElevenLabs
    const userText = await transcribeAudio(audio);

    if (!userText || userText.trim().length === 0) {
      return NextResponse.json(
        { error: 'Aucun texte d√©tect√© dans l\'audio' },
        { status: 400 }
      );
    }

    // 3. R√©cup√©rer le contexte utilisateur pour Claude
    const aiDouble = await db
      .select()
      .from(aiDoubles)
      .where(eq(aiDoubles.userId, userIdNum))
      .limit(1);

    const user = await db
      .select()
      .from(users)
      .where(eq(users.id, userIdNum))
      .limit(1);

    // 4. G√©n√©rer la r√©ponse avec Claude
    const aiResponse = await generateClaudeResponse(
      userText,
      '', // personnalit√© par d√©faut
      aiDouble[0] || null,
      user[0]?.name || ''
    );

    // 5. Text-to-Speech avec ElevenLabs
    // Voix par d√©faut: "Rachel" (voix fran√ßaise naturelle)
    const DEFAULT_VOICE_ID = '21m00Tcm4TlvDq8ikWAM';
    let voiceId = aiDouble[0]?.voiceId;

    // V√©rifier si le voiceId est valide (pas un mock)
    if (!voiceId || voiceId.includes('mock') || voiceId.includes('test')) {
      voiceId = DEFAULT_VOICE_ID;
    }
    let audioUrl: string | null = null;

    console.log('üé§ Utilisation voiceId:', voiceId);
    console.log('üìù Texte √† synth√©tiser:', aiResponse.substring(0, 100) + '...');
    audioUrl = await generateTTS(aiResponse, voiceId);

    if (!audioUrl) {
      return NextResponse.json(
        { error: 'Erreur lors de la g√©n√©ration audio' },
        { status: 500 }
      );
    }

    // 6. Lancer le job Wav2Lip (OBLIGATOIRE si audioUrl existe)
    let jobId: string | null = null;
    let wav2lipApiUrl: string | null = null;
    let wav2lipError: string | null = null;

    // Utiliser avatar-1.png depuis le dossier public
    const baseUrl = process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000';
    const avatarPhotoUrl = `${baseUrl}/avatar-1.png`;

    // üöÄ APPEL OBLIGATOIRE √† Wav2Lip (pas de condition qui bloque)
    console.log('üöÄ CALL WAV2LIP');
    console.log('[Wav2Lip] Photo:', avatarPhotoUrl);
    console.log('[Wav2Lip] Audio:', audioUrl);
    
    try {
      const wav2lipResult = await generateWav2LipVideo(avatarPhotoUrl, audioUrl);
      console.log('[Wav2Lip] R√©sultat:', wav2lipResult);

      if (wav2lipResult.success && wav2lipResult.jobId) {
        jobId = wav2lipResult.jobId;
        wav2lipApiUrl = wav2lipResult.apiUrl || null;
        console.log('[Wav2Lip] ‚úÖ Job lanc√© - jobId:', jobId, 'apiUrl:', wav2lipApiUrl);
      } else {
        wav2lipError = wav2lipResult.error || 'Erreur inconnue';
        console.error('[Wav2Lip] ‚ùå Erreur:', wav2lipError);
      }
    } catch (error) {
      wav2lipError = error instanceof Error ? error.message : 'Erreur inconnue';
      console.error('[Wav2Lip] ‚ùå Exception:', error);
    }

    // ‚ö†Ô∏è IMPORTANT: Toujours retourner m√™me si Wav2Lip √©choue (pour debug)
    console.log('[Wav2Lip] √âtat final - jobId:', jobId, 'wav2lipApiUrl:', wav2lipApiUrl);

    // R√©cup√©rer l'usage
    const updatedUsage = await getOrCreateUsage(userIdNum);

    // Retourner job_id pour que le frontend fasse le polling
    return NextResponse.json({
      success: true,
      userText,
      aiResponse,
      audioUrl,
      // Pour le polling frontend
      jobId,
      wav2lipApiUrl,
      usageRemaining: updatedUsage.remainingSeconds,
      wav2lipError,
    });

  } catch (error) {
    console.error('Erreur conversation avatar-visio:', error);
    const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
    return NextResponse.json(
      { error: `Erreur: ${errorMessage}` },
      { status: 500 }
    );
  }
}

// Speech-to-Text avec ElevenLabs
async function transcribeAudio(audioFile: File): Promise<string> {
  const elevenlabsApiKey = process.env.ELEVENLABS_API_KEY;

  if (!elevenlabsApiKey) {
    throw new Error('API Key ElevenLabs non configur√©e');
  }

  const formData = new FormData();
  formData.append('file', audioFile);
  formData.append('model_id', 'scribe_v1');

  const response = await fetch('https://api.elevenlabs.io/v1/speech-to-text', {
    method: 'POST',
    headers: {
      'xi-api-key': elevenlabsApiKey,
    },
    body: formData,
  });

  if (!response.ok) {
    const error = await response.text();
    console.error('Erreur ElevenLabs STT:', error);
    throw new Error('Erreur lors de la transcription audio');
  }

  const result = await response.json();
  return result.text || '';
}

// Text-to-Speech avec ElevenLabs + upload vers Blob
async function generateTTS(text: string, voiceId: string): Promise<string> {
  const elevenlabsApiKey = process.env.ELEVENLABS_API_KEY;

  if (!elevenlabsApiKey) {
    throw new Error('ELEVENLABS_API_KEY non configur√©e dans .env.local');
  }

  console.log('TTS - G√©n√©ration audio avec voix:', voiceId);
  console.log('TTS - Texte:', text.substring(0, 100));

  const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`, {
    method: 'POST',
    headers: {
      'xi-api-key': elevenlabsApiKey,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      text,
      model_id: 'eleven_multilingual_v2',
      voice_settings: {
        stability: 0.5,
        similarity_boost: 0.75,
      },
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error('Erreur ElevenLabs TTS:', response.status, errorText);
    throw new Error(`ElevenLabs TTS erreur ${response.status}: ${errorText}`);
  }

  const audioBlob = await response.blob();
  console.log('TTS - Audio g√©n√©r√©, taille:', audioBlob.size);

  // Uploader vers Vercel Blob
  const audioFile = new File([audioBlob], `tts-${Date.now()}.mp3`, { type: 'audio/mpeg' });
  const audioUrl = await uploadToBlob(audioFile, `avatar-visio/audio/${Date.now()}-response.mp3`);
  console.log('TTS - Audio upload√©:', audioUrl);

  return audioUrl;
}

// G√©n√©rer la r√©ponse avec Claude
async function generateClaudeResponse(
  userMessage: string,
  personalityPrompt: string,
  aiDouble: any | null,
  userName: string
): Promise<string> {
  // Construire le system prompt pour l'avatar visio
  let systemPrompt = `Tu es un avatar IA en conversation vid√©o avec l'utilisateur.
Tu dois r√©pondre de mani√®re naturelle, conversationnelle et engageante.

R√àGLES IMPORTANTES:
- R√©ponses COURTES et NATURELLES (2-4 phrases max)
- Parle comme dans une vraie conversation vid√©o
- Sois chaleureux et expressif
- Utilise un langage oral naturel
- Pas de listes √† puces ni de formatage markdown
- Pas d'√©mojis (tu es en vid√©o, ton visage exprime les √©motions)`;

  if (userName) {
    systemPrompt += `\n\nL'utilisateur s'appelle ${userName}. Utilise son pr√©nom naturellement.`;
  }

  if (personalityPrompt) {
    systemPrompt += `\n\n# PERSONNALIT√â DE L'AVATAR\n${personalityPrompt}`;
  }

  // Ajouter le contexte psychologique si disponible
  if (aiDouble?.mbtiType || aiDouble?.enneagramType) {
    systemPrompt += '\n\n# CONTEXTE UTILISATEUR';
    if (aiDouble.mbtiType) systemPrompt += `\nType MBTI: ${aiDouble.mbtiType}`;
    if (aiDouble.enneagramType) systemPrompt += `\nEnn√©agramme: ${aiDouble.enneagramType}`;
  }

  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 256, // R√©ponses courtes pour la vid√©o
    system: systemPrompt,
    messages: [
      {
        role: 'user',
        content: userMessage,
      },
    ],
  });

  return response.content[0].type === 'text' ? response.content[0].text : '';
}
